# Import necessary libraries and mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Install transformers library
!pip install -q transformers

# Import necessary libraries
import json
import re
from typing import Optional
from os import listdir
from os.path import isfile, join
from tqdm.auto import tqdm
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import ViltConfig, ViltProcessor, ViltForQuestionAnswering

# Load questions data from JSON file
f = open('/content/OpenEnded_abstract_v002_val2017_questions.json')
data_questions = json.load(f)
questions = data_questions['questions']

print("Number of questions:", len(questions))

# Print the first question
print(questions[0])

# Load annotations data from JSON file
f = open('/content/abstract_v002_val2017_annotations.json')
data_annotations = json.load(f)
annotations = data_annotations['annotations']

print("Number of annotations:", len(annotations))

# Print the first annotation
print(annotations[0])

# Define a regular expression to extract image IDs from filenames
filename_re_compile = re.compile(r".*(\d{12})\.((jpg)|(png))")

# Define a function to extract image IDs from filenames
def id_from_filename(filename: str) -> Optional[int]:
    """
    Extracts the image ID from a filename.

    Args:
        filename: The filename to extract the ID from.

    Returns:
        The extracted image ID, or None if the filename does not match the pattern.
    """
    match = filename_re_compile.fullmatch(filename)
    if match is None:
        return None
    return int(match.group(1))

# Create a list of filenames in the output directory
root = '/content/drive/MyDrive/scene_img_abstract_v002_binary_val2017/output_directory'
file_names = [f for f in tqdm(listdir(root)) if isfile(join(root, f))]

# Create a mapping from filenames to image IDs
filename_to_id = {root + "/" + file: id_from_filename(file) for file in file_names}
id_to_filename = {v: k for k, v in filename_to_id.items()}

# Define a function to get the score for an answer
def get_score(count: int) -> float:
    return min(1.0, count / 3)

# Process annotations to extract labels and scores
for annotation in tqdm(annotations):
    answers = annotation['answers']
    answer_count = {}
    for answer in answers:
        answer_ = answer["answer"]
        answer_count[answer_] = answer_count.get(answer_, 0) + 1
    labels = []
    scores = []
    for answer in answer_count:
        if answer not in list(config.label2id.keys()):
            continue
        labels.append(config.label2id[answer])
        score = get_score(answer_count[answer])
        scores.append(score)
    annotation['labels'] = labels
    annotation['scores'] = scores

# Print the processed annotations
print(annotations[0])

# Define a custom dataset class for VQA data
class VQADataset(Dataset):
    def __init__(self, annotations, questions, id_to_filename):
        self.annotations = annotations
        self.questions = questions
        self.id_to_filename = id_to_filename

    def __getitem__(self, idx):
        annotation = self.annotations[idx]
        question = self.questions[idx]
        image = Image.open(self.id_to_filename[annotation['image_id']])
        text = question['question']
        labels = annotation['labels']
        return {'image': image, 'text': text, 'labels': labels}

    def __len__(self):
        return len(self.annotations)

# Create a ViltConfig instance
config = ViltConfig.from_pretrained("dandelin/vilt-b32-finetuned-vqa")

# Create a ViltProcessor instance
processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")

# Create a dataset instance
dataset = VQADataset(questions=questions[:100], annotations=annotations[:100], id_to_filename=id_to_filename)

# Create a data loader instance
collate_fn = lambda batch: {'input_ids': torch.tensor([f['input_ids'] for f in batch]), 
                            'attention_mask': torch.tensor([f['attention_mask'] for f in batch]), 
                            'token_type_ids': torch.tensor([f['token_type_ids'] for f inbatch]),
                            'pixel_values': torch.tensor([f['image'].convert('RGB').resize((384, 384)).convert('L') for f in batch]),
                            'labels': torch.tensor([f['labels'] for f in batch])}
train_dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)

# Create a ViltForQuestionAnswering instance
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-mlm", id2label=config.id2label, label2id=config.label2id).to(device)

# Define a training loop
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

model.train()
for epoch in range(50):  # loop over the dataset multiple times
   print(f"Epoch: {epoch}")
   for batch in tqdm(train_dataloader):
        # get the inputs;
        batch = {k:v.to(device) for k,v in batch.items()}

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model(**batch)
        loss = outputs.loss
        print("Loss:", loss.item())
        loss.backward()
        optimizer.step()

# Test the model on a single example
example = dataset[0]
print(example.keys())

# Preprocess the example
example = {k: v.unsqueeze(0).to(device) for k,v in example.items()}

# Forward pass
outputs = model(**example)

# Get the predicted classes
logits = outputs.logits
predicted_classes = torch.sigmoid(logits)

# Get the top 5 predicted classes
probs, classes = torch.topk(predicted_classes, 5)

# Print the top 5 predicted classes and their probabilities
for prob, class_idx in zip(probs.squeeze().tolist(), classes.squeeze().tolist()):
  print(prob, model.config.id2label[class_idx])
